#!/usr/bin/env python
############################################################################
##
## Copyright (c) 2012-2024 60East Technologies Inc., All Rights Reserved.
##
## This computer software is owned by 60East Technologies Inc. and is
## protected by U.S. copyright laws and other laws and by international
## treaties.  This computer software is furnished by 60East Technologies
## Inc. pursuant to a written license agreement and may be used, copied,
## transmitted, and stored only in accordance with the terms of such
## license agreement and with the inclusion of the above copyright notice.
## This computer software or any other copies thereof may not be provided
## or otherwise made available to any other person.
##
## U.S. Government Restricted Rights.  This computer software: (a) was
## developed at private expense and is in all respects the proprietary
## information of 60East Technologies Inc; (b) was not developed with
## government funds; (c) is a trade secret of 60East Technologies Inc.
## for all purposes of the Freedom of Information Act; and (d) is a
## commercial item and thus, pursuant to Section 12.212 of the Federal
## Acquisition Regulations (FAR) and DFAR Supplement Section 227.7202,
## Government's use, duplication or disclosure of the computer software
## is subject to the restrictions set forth by 60East Technologies Inc.
##
############################################################################

###
### This is a tool to binary-search journals for a message with a given bookmark
### or txid, and do so much faster than dumping every journal.
###

import glob, os, sys, math
from signal import signal, SIGINT, SIGPIPE, SIG_DFL
import gzip
import json

def die(error):
    sys.stderr.write("Error: %s\n" % error)
    sys.exit(1)

# Before doing anything, check that we're running at least version 2.5
if ''.join(map(str, sys.version_info[0:3])) < '250':
    die('Error: %s requires Python 2.5 or greater.\n' % sys.argv[0])

# Insert the lib directory into PYTHONPATH
sys.path.append(os.path.join(os.path.dirname(sys.argv[0]), "lib"))
import amps_journal
import optparse

def get_sort_key(filepath):
    """ Predicate function to extract sort key to order journals """
    _, filename = os.path.split(filepath)
    basename, _ = os.path.splitext(filename)
    return basename

def extract_sequence(bookmark_index_data):
    """ Function to get sequence number from bookmark index metadata entry """
    return bookmark_index_data['s']

def first_for_client(filename, clientNameHash, opts):
    print("open {0}".format(filename))
    journal = amps_journal.open_file(filename)
    for record in journal.records(is_redact_data=opts.omit_data):
        if record["clientNameHash"] == clientNameHash:
            journal.close()
            return record
    journal.close()
    return None

def check_index(index_file, clientNameHash, clientSeq):
    open_kwargs = {'encoding': 'iso-8859-1'} if sys.version_info[0] >= 3 else {}
    print("open {0}".format(index_file))
    # NOTE: Lack of "with" syntax here, in order to support older python2 versions
    index_fp = gzip.open(index_file, 'rt', **open_kwargs)
    doc = json.load(index_fp)
    for client_metadata in doc['bookmark_index']['data']:
        if clientNameHash == int(client_metadata['client_hash']):
            min_seq = int(client_metadata['min_seq'])
            max_seq = int(client_metadata['max_seq'])
            index_fp.close()
            # Find matching journal or reduce the search set
            if min_seq <= clientSeq <= max_seq:
                return 0
            elif clientSeq < int(client_metadata['min_seq']):
                return -1
            return 1
    index_fp.close()
    # No entry in this index for clientNameHash
    return None

def get_index_file(files, basename):
    for filename in files:
        if filename.startswith(basename) and filename.endswith('index.gz'):
            return filename
    return None

def check_journal(jnl_filepath, clientNameHash, clientSeq, opts):
    journal = amps_journal.open_file(jnl_filepath)
    print("open {0}".format(jnl_filepath))
    last_clientSeq = None
    entry = -1
    for record in journal.records(is_redact_data=opts.omit_data):
        if record['typeStr'] != 'noop':
            entry += 1
        if clientNameHash == record["clientNameHash"]:
            if record["clientSeq"] == clientSeq:
                journal.close()
                record['entryCount'] = entry
                return (record, 0)
            elif record["clientSeq"] > clientSeq:
                journal.close()
                return (None, -1)
            else:
                last_clientSeq = record["clientSeq"]

    journal.close()
    if last_clientSeq is not None:
        return (None, 1)
    # No entries found for our clientNameHash
    return (None, None)

def bisect(jnl_files, clientNameHash, clientSeq, opts):
    upper_bound = len(jnl_files) - 1
    lower_bound = 0
    # Look mid-way through remaining set of journals
    current_index = int(math.floor((upper_bound - lower_bound) / 2))
    dir_files = {}

    while len(jnl_files):
        # Do we have any entries for the client? Check if index is available
        jnl_filepath = jnl_files[current_index]
        dirname, filename = os.path.split(jnl_filepath)
        if not len(dirname):
            dirname = '.'
        basename, ext = os.path.splitext(filename)
        if ext is None:
            basename = filename

        index_file = None
        # Check if directory file list is cached
        if not opts.ignore_indexes:
            if dirname not in dir_files:
                # Py3 can use optimized os.scandir method
                if 'scandir' in dir(os):
                    with os.scandir(dirname) as it:
                        dir_files[dirname] = [f.name for f in it]
                else:
                    for _, _, files in os.walk(dirname):
                        dir_files[dirname] = files
                        break
            index_file = get_index_file(dir_files[dirname], basename)

        if index_file is not None:
            index_result = check_index(os.path.join(dirname, index_file), clientNameHash, clientSeq)
            if index_result is None:
                # No entries for our clientNameHash in this journal; shrink list
                del jnl_files[current_index]
                upper_bound -= 1
                current_index = int(math.floor((upper_bound - lower_bound) / 2))
            elif index_result == 0:
                # Per the index, this should be our journal
                record, result = check_journal(jnl_filepath, clientNameHash, clientSeq, opts)
                return "Not found." if record is None else record
            elif index_result < 0:
                if current_index == lower_bound:
                    break
                upper_bound = current_index - 1
                delta = upper_bound - lower_bound
                if delta < 0:
                    break
                elif delta < 1:
                    current_index = upper_bound
                else:
                    current_index = upper_bound - int(math.floor(delta / 2))
            else:
                if current_index == upper_bound:
                    break
                # The sequence number we're looking for is greater than our current journal
                lower_bound = current_index + 1
                delta = upper_bound - lower_bound
                if delta < 0:
                    break
                elif delta < 1:
                    current_index = lower_bound
                else:
                    current_index = lower_bound + int(math.floor(delta / 2))
        else:
            # No corresponding index file; must search journal
            record, result = check_journal(jnl_filepath, clientNameHash, clientSeq, opts)
            if result is None:
                # No entries for our clientNameHash in this journal; shrink list
                del jnl_files[current_index]
                upper_bound -= 1
                current_index = int(math.floor((upper_bound - lower_bound) / 2))
            elif result == 0:
                return record
            elif result < 0:
                if current_index == lower_bound:
                    break
                upper_bound = current_index - 1
                delta = upper_bound - lower_bound
                if delta < 0:
                    break
                elif delta < 1:
                    current_index = upper_bound
                else:
                    current_index = upper_bound - int(math.floor(delta / 2))
            else:
                if current_index == upper_bound:
                    break
                # The sequence number we're looking for is greater than our current journal
                lower_bound = current_index + 1
                delta = upper_bound - lower_bound
                if delta < 0:
                    break
                elif delta < 1:
                    current_index = lower_bound
                else:
                    current_index = lower_bound + int(math.floor(delta / 2))
    return "Not found."

def search_bookmark(files, bookmark, opts):
    parts = bookmark.split('|')
    clientNameHash = int(parts[0])
    clientSeq = int(parts[1])
    return bisect(files, clientNameHash, clientSeq, opts)

def search_txid(files, pat, opts):
    txid = 0
    try:
        txid = int(pat)
    except:
        return "Invalid transaction id or bookmark: %s"%pat

    entry = -1 # We pre-increment and want the first entry to be 0
    for filename in files:
        journal = amps_journal.open_file(filename)
        extents = journal.getExtents()
        if txid >= extents[0] and (txid <= extents[2] or extents[2] == 0):
            print("should be in {0}".format(filename))
            for record in journal.records(is_redact_data=opts.omit_data):
                if record['typeStr'] != 'noop':
                    entry += 1
                if record["localTxId"] == txid:
                    journal.close()
                    record['entryCount'] = entry
                    return record
            journal.close()
            return "Not found."
    return "Not found -- no journal seems to cover that txid."

def search_data(files, pat, opts):
    clientSeq = 0
    clientNameHash = 0
    isBookmark = False
    if pat.find('|') != -1:
        try:
            parts = pat.split('|')
            clientNameHash = int(parts[0])
            clientSeq = int(parts[1])
            isBookmark = True
        except:
            print("warning: pattern contains pipe delimiter, but could not be parsed as a bookmark.")

    # The data and key fields can be Python3 bytes objects
    pat_bytes = pat.encode('UTF-8')
    entry = -1
    for filename in files:
        print("==={0}".format(filename))
        journal = amps_journal.open_file(filename)
        for record in journal.records(is_redact_data=opts.omit_data):
            if record['typeStr'] != 'noop':
                entry += 1
            # Dynamically determine the type of each field to be searched
            if ('data' in record and (
                    (isinstance(record['data'], bytes) and record['data'].find(pat_bytes) != -1)
                    or (isinstance(record['data'], str) and record['data'].find(pat) != -1)
                    or (sys.version_info[0] == 2 and isinstance(record['data'], unicode) and record['data'].decode().find(pat) != -1))
            ) or ('key' in record and (
                (isinstance(record['key'], bytes) and record['key'].find(pat_bytes) != -1)
                or (isinstance(record['key'], str) and record['key'].find(pat) != -1)
                or (sys.version_info[0] == 2 and isinstance(record['key'], unicode) and record['key'].decode().find(pat) != -1))
            ) or (isBookmark
                  and record["clientNameHash"] == clientNameHash
                  and record["clientSeq"] == clientSeq):
                record['entryCount'] = entry
                print(record)
        journal.close()

def search_client(files, client_hashes, opts):
    for filename in files:
        print("===" + filename)
        journal = amps_journal.open_file(filename)
        for record in journal.records(is_redact_data=opts.omit_data):
            if record["clientNameHash"] in client_hashes:
                print(record)
        journal.close()

def search_topic(files, pat, opts):
    for filename in files:
        print("==={0}".format(filename))
        journal = amps_journal.open_file(filename)
        for record in journal.records(is_redact_data=opts.omit_data):
            if ('topic' in record and record['topic'].find(pat) != -1):
                print(record)
        journal.close()

if __name__ == "__main__":
    # Ignore SIGPIPE signal for valid use cases such as piping output to "head"
    signal(SIGPIPE, SIG_DFL)
    signal(SIGINT, SIG_DFL)

    usage = """
amps_journal_search --data <search-string> <journal-files>
amps_journal_search <search-pattern> <journal-files>

--data: search for all records containing <search-string>, and if <search-string> is a bookmark,
        also include records matching this bookmark.

<search-pattern>: one of
  bookmark:        search the journals for a message with the given client name hash and sequence;
  transaction id:  search for a message with the given local transaction id

<journal-files>: list of .journal or .journal.gz files to be searched.
    """

    parser = optparse.OptionParser(usage=usage)
    parser.add_option("--data", action="store", dest="full_search", default=None,
                      help="Search all records and metadata for search term.")
    parser.add_option("--topic", action="store", dest="topic_search", default=None,
                      help="Search all metadata for topic search term.")
    parser.add_option("--no-data", action="store_true", dest="omit_data", default=False,
                      help="Do not display message data in the journal output.")
    parser.add_option("--ignore-index-files", action="store_true", dest="ignore_indexes", default=False,
                      help="If index files are present, don't use their metadata.")
    parser.add_option("--client", action="append", type="int", dest="client_hash", default=None,
                      help="Search client hash(es) metadata for all matching records. " +
                      "This argument may be provided multiple times to search for multiple clients.")
    (opts, args) = parser.parse_args()

    # Ensure a search term (via --data or without) and at least one journal is provided
    if ((opts.full_search is None \
        and opts.client_hash is None \
        and opts.topic_search is None \
        and len(args) < 2) \
       or(opts.full_search is not None and len(args) < 1) \
       or(opts.client_hash is not None and len(args) < 1) \
       or(opts.topic_search is not None and len(args) < 1)):
        print(args)
        parser.print_help()
        sys.exit(1)
    if (opts.full_search is None
        and opts.client_hash is None
        and opts.topic_search is None):
        pat = args.pop(0)
    files = args

    ## Allow the journal files to be specified in any order and reorder them by serial.
    files = sorted(files, key=get_sort_key)

    if opts.full_search is not None:
        search_data(files, opts.full_search, opts)
    elif opts.topic_search is not None:
        search_topic(files, opts.topic_search, opts)
    elif opts.client_hash is not None:
        search_client(files, opts.client_hash, opts)
    elif pat.find('|') == -1:
        print(search_txid(files, pat, opts))
    else:
        print(search_bookmark(files, pat, opts))
